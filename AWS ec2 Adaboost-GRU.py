{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AWS EC2_Adaboost-GRU.py",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPMEWPn3vpOVcEYVE/hxJZH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sujin-create/hodlhodl/blob/Price-Prediction/AWS%20ec2%20Adaboost-GRU.py\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_d_-gB3rcVe"
      },
      "outputs": [],
      "source": [
        "import pyupbit\n",
        "import pandas as pd\n",
        "import time\n",
        "import datetime \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, GRU, Dropout, LSTM, InputLayer\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "\n",
        "# vix지수, 구글트렌트 지수 불러오기 위해 추가한 라이브러리\n",
        "import pymysql \n",
        "import schedule\n",
        "import yfinance as yf\n",
        "from datetime import datetime\n",
        "import pytrends\n",
        "from pytrends.request import TrendReq\n",
        "\n",
        "def predict_start():\n",
        "    # google trend\n",
        "    keyword = [\"bitcoin\"]   # Bitcoin 키워드의 데이터를 가져옴\n",
        "    startdate = \"2017-09-25\"\n",
        "    enddate = str(datetime.today()).split(\" \")[0]\n",
        "    timerange = startdate + \" \" + enddate   # 시작날짜 ~ 오늘날짜까지의 데이터를 가져옴\n",
        "\n",
        "    result = pd.DataFrame()\n",
        "    pytrends = TrendReq()\n",
        "    pytrends.build_payload(kw_list=keyword, timeframe=timerange)\n",
        "    trends = pytrends.interest_over_time()\n",
        "\n",
        "    trends = trends[[\"bitcoin\"]]\n",
        "    trends = trends.reset_index()\n",
        "    trends = trends.rename(columns={\"date\": \"Date\", \"bitcoin\": \"trends\"})\n",
        "    \n",
        "    # VIX\n",
        "    final = datetime.today()\n",
        "    vix = yf.download('^VIX', interval='1d', start=\"2017-09-25\", end=final)['Close']\n",
        "    vix = pd.DataFrame(vix)\n",
        "    vix_close = vix.rename(columns={\"Close\": \"vix_close\"})\n",
        "\n",
        "    # upbit\n",
        "    ticker = \"KRW-BTC\"\n",
        "    df = pyupbit.get_ohlcv(ticker, interval=\"day\", count = 3650).reset_index() # count=200(default)\n",
        "    df_org = df.copy()\n",
        "\n",
        "    # datetime 형태의 Date 컬럼\n",
        "    df.reset_index(inplace=True)\n",
        "    df = df.rename(columns={\"index\": \"Date\"})\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"Date\"].dt.date)\n",
        "\n",
        "    # vix 종가를 df에 결합 \n",
        "    df = pd.merge(df, vix_close, on=\"Date\", how=\"left\")\n",
        "\n",
        "    # 공휴일 vix 가격을 전날 가격으로 대치 \n",
        "    df = df.fillna(method = \"ffill\")\n",
        "\n",
        "    # trends 데이터를 df에 결합\n",
        "    df = pd.merge(df, trends, on=\"Date\", how=\"outer\")\n",
        "\n",
        "    # 주간 트렌드를 일별로 채우기\n",
        "    df = df.fillna(method=\"ffill\")\n",
        "    df = df.fillna(method=\"bfill\")\n",
        "    df2 = df\n",
        "\n",
        "    # smoothing by MA\n",
        "    df2[\"close_ma5\"] = df2[\"close\"].rolling(window=5).mean()\n",
        "    df2[\"close_ma10\"] = df2[\"close\"].rolling(window=10).mean()\n",
        "    df2[\"close_ma20\"] = df2[\"close\"].rolling(window=20).mean()\n",
        "    df2[\"close_ma60\"] = df2[\"close\"].rolling(window=60).mean()\n",
        "\n",
        "    df2[\"vix_close_ma5\"] = df2[\"vix_close\"].rolling(window=5).mean()\n",
        "    df2[\"vix_close_ma10\"] = df2[\"vix_close\"].rolling(window=10).mean()\n",
        "    df2[\"vix_close_ma20\"] = df2[\"vix_close\"].rolling(window=20).mean()\n",
        "    df2[\"vix_close_ma60\"] = df2[\"vix_close\"].rolling(window=60).mean()\n",
        "\n",
        "    df = df2[[\"close_ma5\", \"close_ma10\", \"close_ma20\", \"close_ma60\", \n",
        "            \"vix_close_ma5\", \"vix_close_ma10\", \"vix_close_ma20\", \"vix_close_ma60\",\n",
        "            \"trends\", \"close\"]]\n",
        "\n",
        "    # ma로 인해 생긴 결측 제거\n",
        "    df = df.dropna()\n",
        "    df_drop = df\n",
        "\n",
        "    warnings.filterwarnings('ignore')\n",
        "\n",
        "    # inverse transform\n",
        "    def restore(row):\n",
        "        return row * (max(df_drop[\"close\"]) - min(df_drop[\"close\"])) + min(df_drop[\"close\"])\n",
        "\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    # 스케일을 적용할 column을 정의\n",
        "    scale_cols = [\"close_ma5\", \"close_ma10\", \"close_ma20\", \"close_ma60\", \n",
        "                \"vix_close_ma5\", \"vix_close_ma10\", \"vix_close_ma20\", \"vix_close_ma60\",\n",
        "                \"trends\", \"close\"]\n",
        "    # 스케일 후 columns\n",
        "    scaled = scaler.fit_transform(df[scale_cols])\n",
        "    df = pd.DataFrame(scaled, columns=scale_cols)\n",
        "    x_train, x_test, y_train, y_test = train_test_split(df.drop('close', 1), df['close'], test_size=0.2, random_state=0, shuffle=False)\n",
        "    df2 = df_org\n",
        "\n",
        "    def windowed_dataset(series, window_size, batch_size, shuffle):\n",
        "        series = tf.expand_dims(series, axis=-1)\n",
        "        ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "        ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "        ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
        "        if shuffle:\n",
        "            ds = ds.shuffle(1000)\n",
        "        ds = ds.map(lambda w: (w[:-1], w[-1]))\n",
        "        return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "    WINDOW_SIZE=10\n",
        "    BATCH_SIZE=32\n",
        "\n",
        "    # trian_data는 학습용 데이터셋, test_data는 검증용 데이터셋 입니다.\n",
        "    train_data = windowed_dataset(y_train, WINDOW_SIZE, BATCH_SIZE, True)\n",
        "    test_data = windowed_dataset(y_test, WINDOW_SIZE, BATCH_SIZE, False)\n",
        "\n",
        "    # 아래의 코드로 데이터셋의 구성을 확인해 볼 수 있습니다.\n",
        "    # X: (batch_size, window_size, feature)\n",
        "    # Y: (batch_size, feature)\n",
        "    for data in train_data.take(1):\n",
        "        print(f'데이터셋(X) 구성(batch_size, window_size, feature갯수): {data[0].shape}')\n",
        "        print(f'데이터셋(Y) 구성(batch_size, window_size, feature갯수): {data[1].shape}')\n",
        "        \n",
        "    # GRU-Adaboost FIT\n",
        "    model = Sequential()\n",
        "    model.add(InputLayer(input_shape=(x_train.shape[1],1)))\n",
        "    model.add(GRU(units=128))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(units=1))\n",
        "\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "    # adaboost\n",
        "    GRU_Predictors = KerasRegressor(build_fn=lambda:model, epochs=5, batch_size=32)   # epoch수 변경\n",
        "    final_model = AdaBoostRegressor(GRU_Predictors, n_estimators=5, random_state=42)\n",
        "\n",
        "    final_model.fit(x_train,y_train)\n",
        "\n",
        "    print(\"예측을 시작합니다.\")\n",
        "    preds = final_model.predict(x_test)\n",
        "    print(\"예측을 종료합니다.\")\n",
        "\n",
        "    df_actu = df_org[-len(preds):]\n",
        "\n",
        "    df_actu[\"pred\"] = restore(preds)       # pred 열에 예측 값 추가\n",
        "    df_actu = df_actu[[\"index\",\"close\", \"pred\"]]\n",
        "\n",
        "    prediction_data = df_actu\n",
        "\n",
        "    prediction_data.reset_index(inplace=True)\n",
        "    prediction_data = prediction_data.rename(columns={\"index\": \"date\"})\n",
        "\n",
        "    prediction_data['date'] = pd.to_datetime(prediction_data['date']).dt.date     # 날짜 뒤에 나오는 시간 제거\n",
        "\n",
        "\n",
        "    # MAE, MSE, RMSE : 시계열 데이터 예측 모델의 성능을 평가하는 지표\n",
        "    def MAE(y_true, y_pred):\n",
        "        return \"{:.2e}\".format(np.mean(np.abs(y_true-y_pred)))\n",
        "\n",
        "    y_true = df_actu['close']\n",
        "    y_pred = df_actu['pred']\n",
        "    MAE_value = MAE(y_true, y_pred)\n",
        "    print(\"MAE = \", MAE_value)\n",
        "\n",
        "    print(\"start adding\")\n",
        "    # prediction값 모두 저장.\n",
        "\n",
        "    conn = pymysql.connect(host='autotrading-db.cjolqhecq70a.ap-northeast-2.rds.amazonaws.com',\n",
        "                        user = 'admin',\n",
        "                        password='AutoTrading1234',\n",
        "                        db = 'AutoTrading',\n",
        "                        charset = 'utf8')\n",
        "    curs = conn.cursor()\n",
        "\n",
        "    sql_row = [(prediction_data['date'][len(preds)-1],prediction_data['close'][len(preds)-1],prediction_data['preds'][len(preds)-1])]\n",
        "\n",
        "\n",
        "    insert_result = \"\"\"insert into Adaboost(prediction_date,close_price,pred_price) values (%s,%s,%s)\"\"\"\n",
        "    curs.executemany(insert_result,sql_row)\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "    \n",
        "    print(\"successfully added\")\n",
        "\n",
        "schedule.every().day.at(\"00:01\").do(lambda: predict_start()) # 한국시간 09:01 => 서버시간 0:01\n",
        "\n",
        "while True:\n",
        "    schedule.run_pending()"
      ]
    }
  ]
}